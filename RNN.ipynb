{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from data_preprocessing import data_preprocessing\n",
    "\n",
    "vocabulary_size = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_units=100, bptt_truncate=4):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # U is the Matric from input to hidden layer\n",
    "        self.U = np.random.uniform(-np.sqrt(1.0/vocab_size), np.sqrt(1.0 / vocab_size), (hidden_units ,vocab_size))\n",
    "        # V is from hidden layer to output\n",
    "        self.V = np.random.uniform(-np.sqrt(1.0/hidden_units), np.sqrt(1.0 / hidden_units), (vocab_size, hidden_units))\n",
    "        # W is the transistion matrix from St-1 to St\n",
    "        self.W = np.random.uniform(-np.sqrt(1.0/hidden_units), np.sqrt(1.0 / hidden_units), (hidden_units, hidden_units))\n",
    "\n",
    "    def feed_forward(self, input):\n",
    "        T = len(input)\n",
    "        s = np.zeros((T + 1, self.hidden_units))\n",
    "        s[-1] = np.zeros(self.hidden_units)\n",
    "        o = np.zeros((T, self.vocab_size))\n",
    "        for t in range(T):\n",
    "            s[t] = np.tanh(self.U[:,input[t]] + self.W.dot(s[t - 1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return o, s\n",
    "\n",
    "    def prediction(self, x):\n",
    "        o, s = self.feed_forward(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0;\n",
    "        for i in range(len(y)):\n",
    "            o, s = self.feed_forward(x[i])\n",
    "            correct_word_prediction = o[np.arange(len(y[i])), y[i]] \n",
    "            L += -1 * np.sum(np.log(correct_word_prediction))\n",
    "        return L\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        N = np.sum([len(yi) for yi in y])\n",
    "        return self.calculate_total_loss(x, y) / N\n",
    "\n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        o, s = self.feed_forward(x)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        \n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.0\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            for bptt_step in np.arange(max(0, t - self.bptt_truncate), t + 1):\n",
    "                dLdW += np.outer(delta_t, s[bptt_step - 1])\n",
    "                dLdU[:, x[bptt_step]] += delta_t\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step - 1] ** 2)\n",
    "        return dLdU, dLdV, dLdW\n",
    "\n",
    "    def numpy_sgd_step(self, x, y, learning_rate=0.005):\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "\n",
    "    def SGD(self, X_train, y_train, learning_rate=0.005, num_epoches=100, evaluate_loss_after=5):\n",
    "        losses = []\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(num_epoches):\n",
    "            print (epoch)\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = self.calculate_total_loss(X_train, y_train)\n",
    "                losses.append(loss)\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print \"%s: loss %d after num_example_see=%d epoch=%d\" %(time, loss,num_examples_seen, epoch)\n",
    "                if (len(losses) > 1):\n",
    "                    if(losses[-1] > losses[-2]):\n",
    "                        learning_rate = learning_rate * 0.5\n",
    "            for i in range(len(y_train)):\n",
    "                self.numpy_sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "                num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(arr):\n",
    "    exp_sum = np.sum(np.exp(arr))\n",
    "    return np.exp(arr) / exp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Proprocessing Input Data\n",
      "Finishing Data Processing\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = data_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2018-08-15 22:38:55: loss 17578 after num_example_see=0 epoch=0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "2018-08-15 22:39:34: loss 13277 after num_example_see=500 epoch=5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "2018-08-15 22:40:13: loss 11152 after num_example_see=1000 epoch=10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "2018-08-15 22:40:51: loss 10722 after num_example_see=1500 epoch=15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "2018-08-15 22:41:29: loss 10527 after num_example_see=2000 epoch=20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "2018-08-15 22:42:10: loss 10391 after num_example_see=2500 epoch=25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "2018-08-15 22:42:49: loss 10292 after num_example_see=3000 epoch=30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "2018-08-15 22:43:30: loss 10181 after num_example_see=3500 epoch=35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "2018-08-15 22:44:10: loss 10093 after num_example_see=4000 epoch=40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "2018-08-15 22:44:51: loss 10006 after num_example_see=4500 epoch=45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocab_size=vocabulary_size)\n",
    "model.SGD(X_data[:100], y_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
